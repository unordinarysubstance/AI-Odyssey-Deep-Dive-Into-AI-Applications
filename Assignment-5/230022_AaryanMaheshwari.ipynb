{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QxlLcnvL_4Dh",
        "outputId": "29f7a92c-8db9-4c9b-8c10-9c2ffbe7ea37"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20, Train Loss: 16.123420727033558\n",
            "Epoch 1/20, Validation Loss: 12.14051950545538\n",
            "Epoch 2/20, Train Loss: 11.746169570033535\n",
            "Epoch 2/20, Validation Loss: 11.501066707429432\n",
            "Epoch 3/20, Train Loss: 11.29504296238437\n",
            "Epoch 3/20, Validation Loss: 11.224675950549898\n",
            "Epoch 4/20, Train Loss: 11.020053892779204\n",
            "Epoch 4/20, Validation Loss: 11.043415887015206\n",
            "Epoch 5/20, Train Loss: 10.786331709177215\n",
            "Epoch 5/20, Validation Loss: 10.90342916761126\n",
            "Epoch 6/20, Train Loss: 10.589723826917403\n",
            "Epoch 6/20, Validation Loss: 10.832428114754814\n",
            "Epoch 7/20, Train Loss: 10.417540971486847\n",
            "Epoch 7/20, Validation Loss: 10.747848329089937\n",
            "Epoch 8/20, Train Loss: 10.258356954422466\n",
            "Epoch 8/20, Validation Loss: 10.652052334376744\n",
            "Epoch 9/20, Train Loss: 10.13851047001002\n",
            "Epoch 9/20, Validation Loss: 10.616367067609515\n",
            "Epoch 10/20, Train Loss: 9.989524648233425\n",
            "Epoch 10/20, Validation Loss: 10.56714271363758\n",
            "Epoch 11/20, Train Loss: 9.902490879129047\n",
            "Epoch 11/20, Validation Loss: 10.493872914995466\n",
            "Epoch 12/20, Train Loss: 9.765917658074502\n",
            "Epoch 12/20, Validation Loss: 10.426063446771531\n",
            "Epoch 13/20, Train Loss: 9.675378934006018\n",
            "Epoch 13/20, Validation Loss: 10.39372139885312\n",
            "Epoch 14/20, Train Loss: 9.551944908188895\n",
            "Epoch 14/20, Validation Loss: 10.412347657339913\n",
            "Epoch 15/20, Train Loss: 9.474891074596007\n",
            "Epoch 15/20, Validation Loss: 10.421214875720796\n",
            "Epoch 16/20, Train Loss: 9.385980530019188\n",
            "Epoch 16/20, Validation Loss: 10.367612339201427\n",
            "Epoch 17/20, Train Loss: 9.265355376378158\n",
            "Epoch 17/20, Validation Loss: 10.395076615469796\n",
            "Epoch 18/20, Train Loss: 9.181462448798806\n",
            "Epoch 18/20, Validation Loss: 10.35538269224621\n",
            "Epoch 19/20, Train Loss: 9.099684530971972\n",
            "Epoch 19/20, Validation Loss: 10.352205094837007\n",
            "Epoch 20/20, Train Loss: 9.017851291258642\n",
            "Epoch 20/20, Validation Loss: 10.31311475662958\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import random\n",
        "\n",
        "# Load data\n",
        "df = pd.read_csv('/content/drive/MyDrive/names.csv')\n",
        "names = df['Name'].astype(str).str.lower().values\n",
        "\n",
        "# Character mapping\n",
        "chars = sorted(set(''.join(names)))\n",
        "char_to_int = {c: i for i, c in enumerate(chars, start=1)}\n",
        "char_to_int['<PAD>'] = 0  # Padding token\n",
        "int_to_char = {i: c for c, i in char_to_int.items()}\n",
        "\n",
        "# Parameters\n",
        "max_length = max(len(name) for name in names)\n",
        "vocab_size = len(char_to_int)\n",
        "\n",
        "# Custom Dataset\n",
        "class NamesDataset(Dataset):\n",
        "    def __init__(self, names, char_to_int, max_length):\n",
        "        self.names = names\n",
        "        self.char_to_int = char_to_int\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.names)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        name = self.names[idx]\n",
        "        x = [self.char_to_int[c] for c in name[:-1]]\n",
        "        y = [self.char_to_int[c] for c in name[1:]]\n",
        "        x = [0] * (self.max_length - len(x)) + x\n",
        "        y = [0] * (self.max_length - len(y)) + y\n",
        "        return torch.tensor(x), torch.tensor(y)\n",
        "\n",
        "# Split the dataset into training, validation, and testing sets\n",
        "def train_val_test_split(names, val_size=0.1, test_size=0.1):\n",
        "    random.shuffle(names)\n",
        "    test_split_idx = int(len(names) * (1 - test_size))\n",
        "    val_split_idx = int(len(names) * (1 - test_size - val_size))\n",
        "    return names[:val_split_idx], names[val_split_idx:test_split_idx], names[test_split_idx:]\n",
        "\n",
        "train_names, val_names, test_names = train_val_test_split(names)\n",
        "\n",
        "# Create datasets and dataloaders\n",
        "train_dataset = NamesDataset(train_names, char_to_int, max_length)\n",
        "val_dataset = NamesDataset(val_names, char_to_int, max_length)\n",
        "test_dataset = NamesDataset(test_names, char_to_int, max_length)\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# RNN Model\n",
        "class NameGeneratorRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(NameGeneratorRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        self.lstm = nn.LSTM(hidden_size, hidden_size)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x, hidden):\n",
        "        x = self.embedding(x)\n",
        "        out, hidden = self.lstm(x, hidden)\n",
        "        out = self.fc(out)\n",
        "        return out, hidden\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        return (torch.zeros(1, batch_size, self.hidden_size), torch.zeros(1, batch_size, self.hidden_size))\n",
        "\n",
        "# Hyperparameters\n",
        "input_size = vocab_size\n",
        "hidden_size = 128\n",
        "output_size = vocab_size\n",
        "\n",
        "model = NameGeneratorRNN(input_size, hidden_size, output_size)\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "epochs = 20\n",
        "for epoch in range(epochs):\n",
        "    model.train()  # Set the model to training mode\n",
        "    total_train_loss = 0\n",
        "    for inputs, targets in train_dataloader:\n",
        "        model.zero_grad()\n",
        "        batch_size = inputs.size(0)\n",
        "        hidden = model.init_hidden(batch_size)  # Initialize hidden with batch size\n",
        "\n",
        "        loss = 0\n",
        "        inputs = inputs.transpose(0, 1)  # For LSTM: seq_len x batch_size\n",
        "        targets = targets.transpose(0, 1)\n",
        "\n",
        "        for i in range(inputs.size(0)):\n",
        "            output, hidden = model(inputs[i].unsqueeze(0), hidden)\n",
        "            loss += criterion(output.squeeze(0), targets[i])\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_train_loss += loss.item()\n",
        "\n",
        "    print(f'Epoch {epoch+1}/{epochs}, Train Loss: {total_train_loss/len(train_dataloader)}')\n",
        "\n",
        "\n",
        "    model.eval()\n",
        "    total_val_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in val_dataloader:\n",
        "            batch_size = inputs.size(0)\n",
        "            hidden = model.init_hidden(batch_size)\n",
        "\n",
        "            loss = 0\n",
        "            inputs = inputs.transpose(0, 1)\n",
        "            targets = targets.transpose(0, 1)\n",
        "\n",
        "            for i in range(inputs.size(0)):\n",
        "                output, hidden = model(inputs[i].unsqueeze(0), hidden)\n",
        "                loss += criterion(output.squeeze(0), targets[i])\n",
        "\n",
        "            total_val_loss += loss.item()\n",
        "\n",
        "    print(f'Epoch {epoch+1}/{epochs}, Validation Loss: {total_val_loss/len(val_dataloader)}')\n",
        "\n",
        "# Generate names\n",
        "def generate_similar_names(seed, max_length=8, num_samples=5):\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "    input_sequence = [char_to_int.get(char, 0) for char in seed]  # Handle unknown chars with padding\n",
        "    generated_names = []\n",
        "\n",
        "    for _ in range(num_samples):\n",
        "        current_name = seed\n",
        "        hidden = model.init_hidden(1)  # Initialize hidden for a single sequence\n",
        "\n",
        "        for i in range(len(seed), max_length):\n",
        "            input_char = torch.tensor([char_to_int.get(current_name[-1], 0)]).unsqueeze(0)\n",
        "            output, hidden = model(input_char, hidden)\n",
        "            output_dist = torch.softmax(output.squeeze(0), dim=1).data\n",
        "            top_i = torch.multinomial(output_dist[-1], 1)[0]\n",
        "\n",
        "            predicted_char = int_to_char[top_i.item()]\n",
        "            if predicted_char == '<PAD>':\n",
        "                break\n",
        "            current_name += predicted_char\n",
        "\n",
        "        generated_names.append(current_name)\n",
        "\n",
        "    return generated_names\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "print(generate_similar_names('aryan', num_samples=3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M8RmYMm6SKzq",
        "outputId": "8e3fdab0-3079-4f1a-89bd-65af29258e7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['aryant)t', 'aryanaze', 'aryandvn']\n"
          ]
        }
      ]
    }
  ]
}